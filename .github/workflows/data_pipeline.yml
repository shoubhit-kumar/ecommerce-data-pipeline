name: E-commerce Data Pipeline

on:
  schedule:
    # Run daily at 2:00 AM UTC (7:30 AM IST)
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual triggers
    inputs:
      days_of_data:
        description: 'Number of days of data to generate'
        required: false
        default: '30'
        type: string

env:
  BUCKET_NAME: ecommerce-pipeline-bucket-471107  # Update this
  PROJECT_ID: ecommerce-pipeline-471107             # Update this
  DATASET_ID: ecommerce_analytics

jobs:
  data_pipeline:
    runs-on: ubuntu-latest
    name: ğŸš€ E-commerce Data Pipeline
    
    steps:
    - name: ğŸ“‚ Checkout Repository
      uses: actions/checkout@v4
      
    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: ğŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas numpy google-cloud-storage google-cloud-bigquery python-dateutil
        
    - name: ğŸ” Authenticate to Google Cloud
      uses: google-github-actions/auth@v1
      with:
        credentials_json: ${{ secrets.GCP_SA_KEY }}
        
    - name: âš™ï¸ Setup Google Cloud SDK
      uses: google-github-actions/setup-gcloud@v1
      
    - name: ğŸ“Š Run Data Extraction
      run: |
        echo "Starting data extraction phase..."
        python scripts/extract.py
      env:
        BUCKET_NAME: ${{ env.BUCKET_NAME }}
        DAYS_OF_DATA: ${{ github.event.inputs.days_of_data || '30' }}
        
    - name: ğŸ”„ Run Data Transformation
      run: |
        echo "Starting data transformation phase..."
        python scripts/transform.py
      env:
        BUCKET_NAME: ${{ env.BUCKET_NAME }}
        PROJECT_ID: ${{ env.PROJECT_ID }}
        DATASET_ID: ${{ env.DATASET_ID }}
        
    - name: ğŸ” Run Data Quality Checks
      run: |
        echo "Running data quality checks..."
        python scripts/data_quality.py
      env:
        PROJECT_ID: ${{ env.PROJECT_ID }}
        DATASET_ID: ${{ env.DATASET_ID }}
        
    - name: ğŸ“ˆ Generate Pipeline Report
      run: |
        echo "Generating pipeline execution report..."
        python scripts/generate_report.py
      env:
        PROJECT_ID: ${{ env.PROJECT_ID }}
        DATASET_ID: ${{ env.DATASET_ID }}
        
    - name: ğŸ“§ Notify on Failure
      if: failure()
      run: |
        echo "Pipeline failed! Check logs for details."
        # You can add email notification or Slack webhook here

  # Optional: Deploy dashboard updates
  deploy_dashboards:
    runs-on: ubuntu-latest
    needs: data_pipeline
    if: success()
    name: ğŸ“Š Update Dashboards
    
    steps:
    - name: ğŸ“‚ Checkout Repository
      uses: actions/checkout@v4
      
    - name: ğŸ¯ Trigger Dashboard Refresh
      run: |
        echo "Dashboard refresh would be triggered here"
        echo "Integration with Fabric/Power BI APIs can be added"
